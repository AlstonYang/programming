{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Scraping JavaScript\n",
    "\n",
    "- JavaScript的主要功能：\n",
    "    1. 收集追踪使用者的資訊\n",
    "    1. 不重刷頁面就可以繳交表單\n",
    "    1. 內嵌多媒體檔案\n",
    "    1. 驅動整個網上遊戲\n",
    "- 甚至是看起來很簡單的網頁，也常包含JavaScript。\n",
    "- JavaScript出現在 stript 標記中。如：\n",
    "  <img src = 'Chap11_01.png'>\n",
    "  \n",
    "## JavaScript簡介\n",
    "\n",
    "- JavaScript是一個輕度有類型(weakly typed)的語言，語法類似C++或Java。\n",
    "  <img src = 'chap11_02.png'>\n",
    "- =======================我是分隔線===================================\n",
    "  <img src = \"Chap11_03.png\">\n",
    "- 常見JavaScript第三方函式庫：(直接使用Python來執行JS很慢，也很耗計算資源，能夠直接剖析而不執行，會很有用處)\n",
    "\n",
    "    1. jQuery：\n",
    "        1. 使用jQuery你會在JS碼中看到：<img src=\"Chap11_04.png\">\n",
    "        1. 因為jQuery會產生：只有執行JS碼後來動態產生的HTML內容，故要特別注意，不要只擷取到執行前的內容。(下面有一節仔細說明此點)\n",
    "        1. jQuery常包括動畫、互動內容、包嵌之多媒體檔案等，可能對擷取產生困擾。\n",
    "    1. Google Analytics：\n",
    "        1. 網路上最常見的追踪使用者工具。\n",
    "        1. 使用谷哥分析工具的網頁，會有：<img src=\"Chap11_05.png\">\n",
    "        1. 使用谷哥分析工具的網頁，在爬取時，不要讓網頁知道你在爬取它，要全然拋棄任何分析用的cookies或乾脆都不要用任何cookies。\n",
    "    1. Google Maps:\n",
    "        1. 在爬取有地點相關資訊的網頁時，了解谷哥地圖的運作讓你可以較容易取得經緯度、甚至地址資料。\n",
    "        1. 谷哥地圖最常用來標示地點的JS碼：<img src=\"Chap11_06.png\">\n",
    "        1. 用Python來擷取一系列的經緯度是很容易的。\n",
    "        1. 也可以用Google工具來從經緯度得到地址：https://developers.google.com/maps/documentation/javascript/examples/geocoding-reverse\n",
    "        \n",
    "## Ajax and Dynamic HTML\n",
    "\n",
    "- 如果繳交表單或是下載資料而未刷新網頁，這個網頁很可能使用Ajax。\n",
    "- Ajax全名是：Asynchronous JavaScript and XML，功能是：在不重新對網頁做要求(request)的情況下，傳送資訊給伺服器或從伺服器取得資料。\n",
    "- Dynamic HTML (DHTML) 是 可以改變該頁的HTML元素的HTML碼/CSS語言：\n",
    "    1. 某個紐只有在滑鼠移過去才出現\n",
    "    1. 按紐才能改變的背景顏色\n",
    "    1. 某個Ajax要求，可以驅使某些新內容載入\n",
    "    1. 跟動畫、多媒體內容無關\n",
    "- 在爬取時，遇到Ajax或/及DHTML，解決之道：\n",
    "    1. 直接爬取JavaScript的內容\n",
    "    1. 使用能執行JavaScript的Python套件，然後爬取你執行JS碼後看到的內容。\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Selenium在Python中執行JavaScript\n",
    "\n",
    "- Selenium本來是設計來做網頁測試用的工具。\n",
    "- 現在，也常用在需要網頁真實呈現(像在瀏覽器中出現一樣)時。\n",
    "- Selenium本身並不包含瀏覽器，故需要呼叫其他瀏覽器。\n",
    "    1. firefox driver: https://github.com/mozilla/geckodriver/releases\n",
    "    1. Chrome driver:http://chromedriver.chromium.org/\n",
    "    1. 把下載的檔案解壓縮，然後複制進你放selenium的環境：C:\\Users\\Admin\\Anaconda3\\envs\\scraping (這是我的環境)\n",
    "- 在anaconda下安裝selenium：\n",
    "    1. conda install -c conda-forge selenium\n",
    "    1. conda install -c conda-forge/label/gcc7 selenium\n",
    "    1. conda install -c conda-forge/label/cf201901 selenium\n",
    "- 我們在以下網頁做測試：http://pythonscraping.com/pages/javascript/ajaxDemo.html。 這個網頁有一些內容，兩秒後，Ajax啟動，會跑出新的內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve!\n",
      "A button to click!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('http://pythonscraping.com/pages/javascript/ajaxDemo.html')\n",
    "time.sleep(3)\n",
    "print(browser.find_element_by_id('content').text)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Explanation\n",
    "\n",
    "- 上面是我們採用的書中的說法。\n",
    "- 我看了之後，有個疑問：我怎麼知道要去找id=content呢？\n",
    "- 如果我們依照之前的做法，在網頁上敲滑鼠右鍵，檢視網頁原始碼，並看不到這些東西。\n",
    "- 這就是為什麼要使用網頁模擬器了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head>\n",
      "<title>Some JavaScript-loaded content</title>\n",
      "<script src=\"../js/jquery-2.1.1.min.js\"></script>\n",
      "\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"content\">Here is some important text you want to retrieve! <p></p><button id=\"loadedButton\">A button to click!</button></div>\n",
      "\n",
      "<script>\n",
      "$.ajax({\n",
      "    type: \"GET\",\n",
      "    url: \"loadedContent.php\",\n",
      "    success: function(response){\n",
      "\n",
      "\tsetTimeout(function() {\n",
      "\t    $('#content').html(response);\n",
      "\t}, 2000);\n",
      "    }\n",
      "  });\n",
      "\n",
      "function ajax_delay(str){\n",
      " setTimeout(\"str\",2000);\n",
      "}\n",
      "</script>\n",
      "\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/ajaxDemo.html\")\n",
    "time.sleep(3)\n",
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve! A button to click!\n"
     ]
    }
   ],
   "source": [
    "# 等待三秒後，網頁模擬器會出現我們真正要的網頁\n",
    "# 這個時候，我們可以用 .page_source()取得真正\n",
    "# 的網頁的原始碼\n",
    "# 這個時候，我們就可以用BeautifulSoup()來取得內容了！\n",
    "# 當然，也可以用webdriver()內建的方便的功能來取得內容\n",
    "# 比如 .find_element_by_id()等\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"http://pythonscraping.com/pages/javascript/ajaxDemo.html\")\n",
    "time.sleep(3) # driver.implicitly_wait()在 jupyter notebook\n",
    "h = driver.page_source\n",
    "bs = BeautifulSoup(h, 'html.parser')\n",
    "print(bs.find('div').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium Selectors\n",
    "\n",
    "- 在上面的例子中，我們使用：.find_element_by_id()來找 JS 碼中的content\n",
    "- 以下做法可以達成相同目的：\n",
    "    1. browser.find_element_by_css_selector('#content')\n",
    "    1. browser.find_element_by_tag_name('div')<br><br>\n",
    "- 如果要選多個成份，就在 element 上加s：\n",
    "    1. browser.find_elements_by_css_selector('#content')\n",
    "    1. browser.find_elements_by_tag_name('div')\n",
    "    1. 以上回傳Python表列<br><br>\n",
    "- 如果想用BeautifulSoup，則：\n",
    "    pageSource = driver.page_source\n",
    "    bs = BeautifulSoup(pageSource,'html.parser')\n",
    "    print(bs.find(id='content').get_text())\n",
    "\n",
    "- 上一格中，如果time.sleep()改為一秒，則會擷取到原來的、改變前的文本。\n",
    "- 用時間來控制，可以有問題，因為，網頁的載入時間影響因素很多。\n",
    "- 更有效率的做法是：持續檢查完全載入的網頁，看看某個特定的成份是否存在，而只有當該成份存在時，才回傳結果。\n",
    "- 上面的測試網頁，Ajax執行後，會出現一個load按紐，故，以之來斷定真正想截取的內容是否已完全出現："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some important text you want to retrieve!\n",
      "A button to click!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('http://pythonscraping.com/pages/javascript/ajaxDemo.html')\n",
    "\n",
    "try:\n",
    "    element = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'loadedButton')))\n",
    "    # <button id='loadedButton'>\n",
    "finally:\n",
    "    print(browser.find_element_by_id('content').text)\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - WebDriverWait()以及expected_conditions()是Selenium中稱為implicit wait的函式。\n",
    " - implicit wait vs. explicit wait:\n",
    "     1. 前者等待某些DOM (domain object model: XML/HTML等的樹狀結構)串的狀態發生才會繼續\n",
    "     1. 後者定義了硬性規定的時間 (如前例中的等三秒)\n",
    "     1. 前者等待的DOM狀態由expected_conditions(縮寫為EC)定義\n",
    "     1. expected_conditions可以是：\n",
    "         1. 某種提示對話框出現\n",
    "         1. 某個成份(如，一個文字框)被放進 selected 狀態。\n",
    "         1. 頁面標題改變，或文本出現於網頁中/於某個特定成分中。\n",
    "         1. 某成份現在在DOM中可以看得到了，或消失了。\n",
    " - expected_conditions中的成分，以locators來標示，並用By物件來使用。\n",
    "     1. EC.presence_of_element_locate((By.ID, 'loadedButton')) # 用ID loadedButton來定義expected_conditions\n",
    "     1. print(browser.find_element(By.ID, 'content').text) # 用 find_element及locators來建立selectors\n",
    "     1. (browser.find_element_by_id('content').text) #與上功能相同\n",
    " - By物件使用下面的locators:\n",
    "     1. ID: 用HTML ID來找\n",
    "     1. CLASS_NAME: 用HTML的class屬性來找\n",
    "     1. CSS_SELECTOR: 用class, id, tag這些標記來找，真正使用的方式為： #idName, .className, tagName\n",
    "     1. LINK_TEXT: 找HTML中的 a 的文本，如：B.LINK_TEXT, \"Next\" (找叫Next的標籤 (label))\n",
    "     1. PARTIAL_LINK_TEXT：同上，但只需部份文本即可\n",
    "     1. NAME: 利用tag的name屬性來找tag，表單時特別好用\n",
    "     1. TAG_NAME: 用tag的名字來找tag\n",
    "     1. XPATH: 用 XPath 表達示(見下)來選擇對應成份"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XPath 語法\n",
    "\n",
    "- XPath語法有四個主要概念：\n",
    "    1. 根節點　vs. 非根節點\n",
    "        1. /div (找在根節點的div)\n",
    "        1. //div (找任何地點的div)\n",
    "    1. 屬性\n",
    "        1. //@href 選擇位於任何位置的href\n",
    "        1. //a[@href='http://google.com'] 選擇所有指向google的連結\n",
    "    1. 利用位置來選擇節點\n",
    "        1. //a[3] 選檔案中第三個連結\n",
    "        1. //table[last()] 選檔案中最後一個表格\n",
    "        1. //a[position()>3]選檔案中前三個連結\n",
    "    1. \\* 任何字元或節點\n",
    "        1. //table/tr/* 所有表格中，tr的所有子代 (用來選擇cell中th及td兩個標記很好用)\n",
    "        1. //div[@\\*] 選擇有任何屬性的div標記\n",
    "- 更多相關資訊，請見：msdn.microsoft.com/en-us/enus/library/ms256471"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一般而言，爬取JS資料並不需要在電腦螢幕上出現一個瀏覽器。\n",
    "- 不過，出現瀏覽器有幾個好處：\n",
    "    1. 除錯：如果程式碼有錯，能看到網頁會比較容易看到錯誤。\n",
    "    1. 有些測試可能只能在某個特定瀏覽器上才能使用。\n",
    "    1. 有些JS碼在不同瀏覽器有有稍微的不同，需要在瀏覽器上執行才看得出來。\n",
    "- 除了Chrome、Firefox外，selenium還支援：\n",
    "    1. safari_driver = webdriver.Safari()\n",
    "    1. ie_driver = webdriver.Ie()\n",
    "    1. 上面的驅動程式要上網蒐尋下載，並放在你的anaconda的scraping環境中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理重新導向(redirect)\n",
    "\n",
    "- 在：http://pythonscraping.com/pages/javasript/redirectDemo1.html 上示範\n",
    "- 你可以用聰明的方法來偵測導新導向：注意某個在原先頁面載入時DOM中的元素，持續呼叫之，一直到selenium回傳：StaleElementReferenceException，因為，當頁面重新導向時，該元素就不會出現在頁面的DOM中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing out after 10 seconds and returning\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "def waitForLoad(driver):\n",
    "    elem = driver.find_element_by_tag_name(\"html\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        if count > 20:\n",
    "            print('Timing out after 10 seconds and returning')\n",
    "            return\n",
    "        time.sleep(.5)\n",
    "        try: \n",
    "            elem = driver.find_element_by_tag_name('html')\n",
    "        except StaleElementReferenceException:\n",
    "            return\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('http://pythonscraping.com/pages/javascript/redirectDemo1.html')\n",
    "waitForLoad(driver)\n",
    "print(len(driver.page_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 也可以寫一個簡單的迴圈，檢查網頁的URL一直到其改變為止或是變成你在找的特定URL。\n",
    "- 等待元素出現或消失是selenium常見的工作，可以使用之前用過的WebDriverWait："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the page you are looking for!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('http://pythonscraping.com/pages/javascript/redirectDemo1.html')\n",
    "try:\n",
    "    bodyElement = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \n",
    "                                                 '//body[contains(text(), \"This is the page you are looking for!\")]')))\n",
    "    print(bodyElement.text)\n",
    "except TimeoutException:\n",
    "    print('Did not find the element')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note\n",
    "\n",
    "- 雖然大部份網頁包含JavaScript，但可能不影響我們爬取資料。\n",
    "- 如果會影響，則可以使用像selenium等工具，來產生你已經會爬取的簡單網頁。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
