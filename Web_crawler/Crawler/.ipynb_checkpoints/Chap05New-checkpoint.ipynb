{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A verb brief introduction to Scrapy\n",
    "\n",
    "- Scrapy 是一個功能強大的、用來做網站爬取的Python模組。\n",
    "- 很多前幾章我們自己下寫的函式功能，Scrapy提供函式庫(library)，供我們使用，直接呼叫。\n",
    "- 不過，因為Scrapy的方便性，要撰寫、執行Scrapy的爬蟲，會受到Scrapy本身的限制，不像之前，直接執行Python碼即可。\n",
    "- 這一節，我們很簡單地介紹Scrapy功能。本節教節內容取自：Scrapy一本就精通，編著：劉碩，出版社：佳魁數位。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝Scrapy\n",
    "\n",
    "- 在Anaconda環境下，打開Anaconda Prompt，啟動scraping這個環境，鍵入：conda install -c conda-forge scrapy\n",
    "- 如果一時三刻想不起來這行指令，google install scrapy on anaconda，打開 Anaconda Cloud那個網頁，就可以找到了。\n",
    "- 如果不是 Anaconda 環境，可在命令提示字元中鍵入：pip install Scrapy。不過，要記得，得先到pip所在的目錄才行。\n",
    "- 不記得上面的指令，也可以google install scrapy，找到Scrapy的主網頁，裏面會提供資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立專案\n",
    "\n",
    "- 在 Anaconda Prompt下，啟動scraping這個環境，鍵入：鍵入： python -m scrapy startproject ScrapyExample f:\\python_practice\\scraping\\\n",
    "- 上行指令中：\n",
    "    1. 如果你直接使用 scrapy startproject ... 出現：fatal error in lauching...，依上所述，加入 python -m\n",
    "    1. startproject 後面，是你的專案的名字，在這裏，我們使用的專案名稱為：ScrapyExample\n",
    "    1. 在專案名稱的後面那串路徑，是你想把專案存到那裏去的路徑。這是可以省略的步驟。如果你在C:\\users\\Admin\\下執行python -m scrapy startproject ...，且沒有標明專案儲存路徑，你的專案會被存在C:\\users\\Admin之下。\n",
    "    1. 如果你已經在你的工作目錄，如：f:\\python_practice\\scraping\\，那麼就不需要標明專案儲存路徑了。\n",
    "    1. 見：<img src = \"scrapy_start.png\">\n",
    "- 執行完後，你可以發現：<img src = 'scrapy02.png'>\n",
    "    1. 在scraping目錄下有一個ScrapyExample，其下又有一個ScrapyExample目錄，在第二層的ScrapyExample，下面會出現許多檔案。\n",
    "    1. 這些檔案是Scrapy幫你建立、撰寫完整的爬蟲會用到，需要在其中增添內容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一隻網頁爬蟲\n",
    "\n",
    "- 這本書，爬取：http://books.toscrape.com 這個網站。\n",
    "- 我們在spiders目錄下，新增一個叫：book_spider.py的程式。\n",
    "- 因為受限於Scrapy的作法，我們換到Spyder去編撰程式。\n",
    "- 為了解釋方便，我們還是把程式貼在下面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BooksSpider(scrapy.Spider):\n",
    "    # 每個爬蟲的唯一標識\n",
    "    name = \"books\"\n",
    "    \n",
    "    # 定義爬蟲爬取的起始點，可以是多個，這裏只有一個\n",
    "    \n",
    "    start_urls = ['http://books.toscrape.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # 分析資料\n",
    "        # 每一本書的資訊在<article class = \"product_pod\">中，\n",
    "        # 我們使用css()方法找到所有這樣的article元素，並依次反覆運算\n",
    "        for book in response.css('article.product_pod'):\n",
    "            # 書名資訊在article>h3>a元素裏的title屬性中\n",
    "            # 下面的xpath指的是HTML tag 間的高低階層路徑，而 . 代表\n",
    "            # article這個標記，標示於： .css()之中\n",
    "            name = book.xpath('./h3/a/@title').extract_first()\n",
    "            \n",
    "            # 書價資訊在<p class=\"price_color\">的文本之中\n",
    "            # 在這裏用兩個冒號來標示文本，因為上面做了一個\n",
    "            # for 迴圈，每個迴圈內只處理一本書的訊息，故\n",
    "            # 只需擷取第一個 <p class = 'price_color'>就好\n",
    "            # 使用 .extract_first()\n",
    "            \n",
    "            price = book.css('p.price_color::text').extract_first()\n",
    "            \n",
    "            # yield功能很像return，但是回傳出一個 generator，\n",
    "            # 那是一種允許iteration但又比表列節省記憶體的物件\n",
    "            \n",
    "            yield {\n",
    "                    'name': name,\n",
    "                    'price': price,}\n",
    "            \n",
    "            # 分析網頁發現，下一頁的url在url.pager > li.next > a 中\n",
    "            # a::attr(href):取得a的屬性href的值的文字\n",
    "        next_url = response.css('ul.pager li.next a::attr(href)').extract_first()\n",
    "    \n",
    "        if next_url:\n",
    "            # 如果找到下一頁的URL，獲得絕對路徑，建置新的Request物\n",
    "            # .urljoin()：藉由 response 自動計算絕對路徑\n",
    "            next_url = response.urljoin(next_url)\n",
    "            yield scrapy.Request(next_url, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簡單說明\n",
    "\n",
    "1. name屬性：一個專案中可能有多個爬蟲，每個爬蟲的name屬性是其唯一標誌。\n",
    "1. start_url屬性：設定一個爬蟲的爬取起始點\n",
    "1. parse方法：當一個頁面下載完成後，Scrapy會回呼一個我們指定的頁面剖析函數來剖析頁面。一個剖析函數通常需完成以下兩個工作：\n",
    "    1. 分析頁面中的資料(Xpath或CSS選擇器)\n",
    "    1. 分析頁面中的連結，並產生對連結頁面的下載請求。\n",
    "    - 頁面剖析函數通常被實現成一個產生器函數，每一項從頁面中分析的資料以及每一個對連結頁面的下載請求都由yield敘述傳送給Scrapy引擎。\n",
    "- 回到Anaconda Prompt，到ScrapExample下執行：python -m scrapy crawl books -o books.csv\n",
    "- csv = comma-separated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy 架構中的各元件\n",
    "\n",
    "1. ENGINE：引擎，架構的核心，其他所有元件在其控制下協作工作，內部元件\n",
    "1. SCHEDULER：排程器，負責對SPIDER傳送的下載請求進行排程，內部元件\n",
    "1. DOWNLOADER：下載器，負責下載頁面(發送HTTP請求/接收HTTP回應)，內部元件\n",
    "1. SPIDER：爬蟲，負責分析頁面中的資料，並產生對新頁面的下載請求，實用者實現\n",
    "1. MIDDLEWARE：中介軟體，負責對Request物件和Response物件進行處理，可選元件\n",
    "1. ITEM PIPELINE：資料管線，負責對爬取到的資料進行處理，可選元件\n",
    "\n",
    "## 架構中的資料流程\n",
    "\n",
    "1. REQUEST：Scrapy中的HTTP請求物件\n",
    "1. RESPONSE: Scrapy中的HTTP回應物件\n",
    "1. ITEM: 從頁面中爬取一項資料\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request物件\n",
    "\n",
    "- Request(url [, callback, method='GET', headers, body, cookies, meta, encoding='utf-8', priority=0, dont_filter=False, errback])\n",
    "- url: 一定要有，請求頁面的URL位址，bytes或str類型\n",
    "- callback: 頁面剖析函數，如未呼叫，則使用預設的Spider剖析方法\n",
    "- method: HTTP的請求方法，預設為GET\n",
    "- headers: HTTP請求的表頭字典，dict類型，如：{'Accept':'text/html', 'User-Agent':'Mozilla/5.0'}。如果某項值為None，表示不發送該項HTTP表頭\n",
    "- body: HTTP 請求的正文，bytes或str類型\n",
    "- cookies: Cookie的資訊字典\n",
    "- meta: Request的中繼資料字典，用於給架構中其他元件傳遞資訊\n",
    "- encoding: 編碼，預設為utf-8\n",
    "- priority: 請求的優先順序，順序高的優先下載\n",
    "- dont_filter: 預設情況下，對同一個url位圵多次傳送下載請求，後面的請求會被送去重篩檢程式過瀘，以避免重覆下載。\n",
    "- errback: 請求出現例外或出現HTTP錯誤時，如404頁面不存在的回呼函數。\n",
    "- Rest的論元雖多，但是一定要有的是：url，很常使用的是：callback。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response物件\n",
    "\n",
    "- Response物件用來描述一個HTTP回應，共有三類。三者差別相當微小。\n",
    "    1. TextResponse\n",
    "    1. HtmlResponse\n",
    "    1. XmlResponse\n",
    "- 以HtmlResponse為例，最常用的屬性有：\n",
    "    1. xpath(query):使用XPath選擇器在Response中分析資料，是response.selector.xpath的捷徑\n",
    "    1. css(query): 使用CSS選擇器在Response中分析資料，是response.selector.css的捷徑\n",
    "    1. urljoin(url): 用於建置絕對url。當傳入的url參數是一個相對位址時，根據response.url計算出對應的絕對url。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider開發流程\n",
    "\n",
    "1. 繼承scrapy.Spider\n",
    "1. 為Spider取名\n",
    "1. 設定爬取起始點\n",
    "1. 實現頁面剖析函數\n",
    "\n",
    "### 繼承scrapy.Spider\n",
    "\n",
    "- Scrapy提供了一個Spider的基礎類別，我們撰寫的Spider需要繼承之\n",
    "- 這個基礎Spider實現了以下內容：\n",
    "    1. 供Scrapy引擎呼叫的介面\n",
    "    1. 供使用者使用的工具程式函數\n",
    "    1. 供使用者存取的屬性\n",
    "- 初學時，不需要在太意scarpy.Spider的內容。有需要時再查文件即可。\n",
    "\n",
    "### 為Spider命名\n",
    "\n",
    "- 因為同一個專案可以用多個Spider，故需為每個Spider制定唯一可供區別的標誌\n",
    "- 執行 scrapy crawl時，使用這個標誌即可。\n",
    "\n",
    "### 設定起爬取點\n",
    "\n",
    "- Spider必定要從某個或某些頁面開始爬取，以start_url來設定。\n",
    "- 雖然我們只設了一個網址，但是由於scrapy.Spider這個類別的函數，我們不用擔心需要用Request去呼叫網頁。\n",
    "- scrapy.Spider提供的parse()只會引起NotImplementError，故，我們在撰寫Spider時，如上面的BooksSpider，必須定義parse()。\n",
    "\n",
    "### 實現頁面剖析函數\n",
    "\n",
    "- 核心部份，需要完成兩項工作：\n",
    "    1. 使用選擇器分析頁面中的資料，將資料封裝後(Item或字典)，傳送給Scrapy引擎。\n",
    "    1. 使用選擇器或LinkExtractor分析頁面中的連結，用其建置新的Request物件並傳送給Scrapy引擎。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 執行scrapy爬蟲\n",
    "\n",
    "- 到你爬蟲所在目錄下，鍵入：scrapy crawl books -o books.csv\n",
    "- 其中，books是你要執行的爬蟲的名稱，在這裏為：books (定義在上面BookSpider這個類別的name中)\n",
    "- \"-o\" 是輸出(output)的意思\n",
    "- books.csv 是輸出的檔案名稱，配合爬蟲名稱，不得任意更動"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
