{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章 Your First Web Scraper\n",
    "\n",
    "- 網頁看起來是由文字、圖型組合的。但是......\n",
    "- 在第一章及第二章，我們要學習看懂這些藏在瀏覽器背後的秘密\n",
    "- 為什麼呢？\n",
    "- 第一章學習內容：\n",
    "    1. 送出GET要求給網頁伺服器中的某一特定頁面，\n",
    "    1. 讀取該頁的HTML輸出，\n",
    "    1. 做點簡單的資料抽取來隔離出你尋找的內容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting\n",
    "\n",
    "- 使用網際網路，瀏覽器會幫你做很多事。\n",
    "- 但想從網路上擷取資料，我們必需了解：(1) 瀏覽器幫你做了什麼(HTML, CSS, JavaScript)；(2) 網路的連接方式\n",
    "- 網路連接的簡圖：假設老張用筆電想連結老李的伺服器...\n",
    "    1. 老張的電腦送出一串1、0的訊號，分別以電壓的高低代表。這些1、0代表以下訊息：<br>\n",
    "        (a) 標頭(header)：包含老張本地路由器的MAC address及老李的IP address <br>\n",
    "        (b) 正文(body)：對老李電腦的請求(request)\n",
    "    2. 老張的路由器把上述請求轉譯為一個從老張的MAC位址傳送到老李的IP的封包，加上該路由器的IP位址當成寄件地址，送到老李的伺服器去。\n",
    "    3. 老張的封包透過一些伺服器，扺達老李的伺服器。\n",
    "    4. 老李的伺服器收到封包。\n",
    "    5. 老李的伺服器讀取封包，並傳給適當的伺服器應用程式。\n",
    "    6. 網頁伺服器收到從伺服器處理器傳來的資料，內容大致為：(a)這是一個GET請求；(b) 要求下列檔案：index.html\n",
    "    7. 網頁伺服器找到相對的網頁，打包成封包，傳回給老張。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上面的幾個步驟，看起來跟瀏覽器無關。\n",
    "- 的確，不用瀏覽器也可以讀取網頁資料。如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 從現在起，要開始把「網頁」想成「檔案」：\n",
    "    1. 現在的網頁，多半連結著許多檔案，如，影像檔、JavaScript檔、CSS檔等。\n",
    "    1. 如，下面的HTML碼，表示要在該網頁檔案相同目錄下取得影像(img)來源(src)檔：cuteKitten.jpg<img src = \"pic_source.png\">\n",
    "- 目前的做法(就是上面程式的from ... import...那裏)，只能開啟特定網頁檔案，還不能取得不同的連結檔\n",
    "- urllib 是標準Python的內建模組(module)，這個模組包含了可以達成下列功能的函式(function)：\n",
    "    1. 從網頁要求(request)資料\n",
    "    1. 處理cookies\n",
    "    1. 甚至改變標頭(header)、使用者代理(user agent)等元資料(meta data)\n",
    "- urlopen可以透過網路開啟一個遠端物件(object)並讀取之。urlopen是一個generic function，可開啟多種檔案，包括：HTML、影像檔及其他檔案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beautifulSoup4簡介\n",
    "\n",
    "- beautifulSoup是一個非內建的Python模組，用處理來HTML、XML等格式的檔案，包括網頁檔。\n",
    "- 安裝模組：\n",
    "    1. 使用Python建立專案，常常會需要建立「虛擬環境」，因為不同專案可能使用不同版本的Python、不同版本的模組，<br>\n",
    "       為了避免版本間的相衝突，故最好依專案建立不同的虛擬環境。\n",
    "    1. 建立這門課的虛擬環境：\n",
    "        (a) 在Anaconda Prompt下，鍵入：conda create --name scraping python=3.7\n",
    "        (b) conda activate scraping\n",
    "        (c) conda install beautifulSoup4 (以及jupyter notebook、spyder)\n",
    "        (d) conda env list (列出所有虛擬環境)\n",
    "        (e) 如果無法使用 conda install Module/Package，可以用anaconda install Module/Package為關鍵字搜尋\n",
    "        (f) 如果找不到，則可使用pip install Module/Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 執行beautifulSoup4\n",
    "\n",
    "- 因為不是內建模組，故需import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BeautifulSoup 是beautifulSoup4 (bs4)中很常用的一個函式。\n",
    "- 上面的碼，跟之前先用urlopen()再做read()有什麼不同呢？\n",
    "- BeautifulSoup提供一個HTML語言剖析器--html.parser，這個剖析器讀進一個HTML檔，得到其結構。\n",
    "- 所以，我們可以直接讀取其下結構的成份，如：bs.h1，可以直接取得html -> body -> h1\n",
    "- 來看一下這個網頁的結構：<img src=\"page1_html.png\">\n",
    "- 同樣是h1，我們可以用以下不同方式來取得其內容。：<br>\n",
    "    bs.html.body.h1 <br>\n",
    "    bs.body.h1 <br>\n",
    "    bs.html.h1 <br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 兩個額外的parsers：\n",
    "1. lxml<br>\n",
    "    (a) 在anaconda prompt下，鍵入：pip (or pip3) install xml <br>\n",
    "    (b) bs = BeautifulSoup(html.read(), 'lxml') <br>\n",
    "    (c) 優點：\n",
    "        (i) 較能處理寫得不好的HTML網頁\n",
    "        (ii) 速度較快 (不是很重要，因為網頁擷取最重要的是網路的速度)\n",
    "    (d) 缺點：\n",
    "        (i) 要另外安裝，並且需要使用第三方C語言程式庫\n",
    "        (ii) 會有程式可攜性的問題\n",
    "2. html5lib <br>\n",
    "    (a) 對寫得不好的HTML網頁容忍性非常高 <br>\n",
    "    (b) bs = BeautifulSoup(html.read(), 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Reliability and Handling Exception\n",
    "\n",
    "- 網頁擷取會出現很多未能事先預見的問題：網頁不存在、HTML格式錯誤，等等...\n",
    "- 在撰寫網頁擷取程式時，要能處理這些錯誤，在Python中叫，例外(Exception)\n",
    "- 比如：在使用urlopen()開啟示範網頁時，最有可能發生的兩種錯誤：\n",
    "    1. 網頁在伺服器上找不到；或是，在讀取時出錯 (404 Page Not Found)\n",
    "    1. 伺服器找不到 (500 Internet Server Error)\n",
    "- 在這些情況下，urlopen()都會回傳HTTPError。\n",
    "- 可以這樣處理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "    # return null, break, or do some other \"plan B\"\n",
    "else:\n",
    "    # program continues. Note: if you return or break in the\n",
    "    # exception catch, youo not need to use the \"else\" statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果根本找不到伺服器，會出現 URLError錯誤。\n",
    "- 我們可以："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('https:\\\\pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print('This server could not be found!')\n",
    "else:\n",
    "    print('It Worked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 即使伺服器沒問題、網頁也存在，仍然有可能找不到你想要的標記(tag)\n",
    "- 使用BeautifulSoup()時，最好檢查一下你要的標記在不在。\n",
    "- 如果不在，BeautifulSoup()會回以None。\n",
    "- 但是，如果你想對None做些什麼事，卻會出現AttributeError！\n",
    "- 故："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'someTag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c0554bc865bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonExisting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonExisting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msomeTag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'someTag'"
     ]
    }
   ],
   "source": [
    "print(bs.nonExisting)\n",
    "print(bs.nonExisting.someTag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tage was not found\n"
     ]
    }
   ],
   "source": [
    "try: badContent = bs.nonExisting.anotherTag\n",
    "except AttributeError as e:\n",
    "        print('Tage was not found')\n",
    "else:\n",
    "        if badContent == None:\n",
    "            print('Tag was not found')\n",
    "        else:\n",
    "            print(badContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整合一下上面所說的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle('http://pythonscraping.com/pages/page1.html')\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在撰寫網頁擷取程式時，最好整體考量一卜你的程式碼，讓你的程式碼同時可以處理例外又具易讀性。\n",
    "- 同時要考慮到程式重覆使用性，不妨定義完整像是getSiteHTML、getTitle這類的函式，可以重覆使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上課練習\n",
    "\n",
    "- 請到books.toscrape.com選擇第一本書：A light in the attic。使用本章的教學內容，取得title的內容。\n",
    "- 仔細看一下該頁原始碼，怎麼樣可以取得該書簡介呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
