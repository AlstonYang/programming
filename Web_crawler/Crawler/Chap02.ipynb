{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced HTML Parsing\n",
    "\n",
    "- 殺雞焉用牛刀。不見得一定要使用進階的HTML剖析。\n",
    "- 不使用HTML剖析，或許可以考慮做暴力的字串搜尋，如 \n",
    "  bs.find_all('table')[4].find_all('tr')[2]findall('td').find_all('div'][1].find('a')\n",
    "- 但這樣做很脆弱，因為，只要網頁做一點小小的更動，你設定的字串模式就被改變了！\n",
    "- 如何能找到你要的資訊，又不用進階HTML剖析呢？有幾個方法：\n",
    "    1. 尋找「列印此頁」這個連結，或是找行動版網頁。\n",
    "    1. 找尋隱藏在JavaScript下的內容。\n",
    "    1. 對頁面標題而言，有用的訊息可能藏在URL中。\n",
    "    1. 找其他提供相關訊息的網頁。\n",
    "- 如果遇到格式不良、埋得很深的資料，不要立刻就著手寫擷取程式。應先考慮其他方式。\n",
    "- 如果沒有其他方式，本章介紹如何根據標記的位置、語境、屬性及內容來找到你想要的標記。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再嘗一口美麗的湯\n",
    "\n",
    "- HTML中，有一些標記可以改變文件內容的某些屬性，如：\n",
    "    <img src=\"CSS_color.png\">\n",
    "- 我們可以看一下：http://www.pythonscraping.com/pages/warandpeace.html\n",
    "- 看一下這個網頁的原始碼，可以看到用上述方法標明字體顏色及其相對應的網頁體現。\n",
    "- 我們可以用類似第一章的方式，把整個網頁擷取下來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "# 使用BeautifulSoup的find_all方式來擷取你想要的標記，並回傳一個表列\n",
    "# findAll是舊的方法名；final_all是符合Python格式要求的新名稱，以下使用新名稱\n",
    "\n",
    "nameList = bs.find_all('span', {'class':'green'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find_all()是一個在撰寫網頁擷取程式時非常常用的、BeautifulSoup()這個函式下的方式。\n",
    "- find_all(name, attrs, recursive, text, limit, **kwargs)，回傳一個表列，其中：\n",
    "    1. name = 標記的名稱\n",
    "    1. attrs = CSS 的分類；注意，1與2必須是Python字串！\n",
    "    1. 如果只找direct children，recursive=False，基礎值是recursive = True，會一直找下去(孫子、曾孫...)\n",
    "    1. text = 某字串，代表搜尋某字串，如：bs.find_all(text='Elsie')\n",
    "    1. limit標明要找幾個結果，如：bs.find_all(\"span\", limit=2)，找前兩個結果\n",
    "    1. **kwargs 代表了對標記的屬性的篩選，如：bs.find_all(id='link2')\n",
    "        結果為 [<a class = \"sister\" ... id=\"link2\">...\n",
    "       1. 所以用kwargs可以做的，都可用本章稍後介紹的方式做，如：bs.find_all(id='text') 等於 bs.find_all('', {'id':'text'})\n",
    "       1. 在找class時，容易出問題，因為class是Python保留字元。所以：bs.find_all(class='green')會產生句法錯誤，要改成：<br>\n",
    "          bs.find_all(class_='green')或是bs.find_all('', {'class':'green'})才可以。\n",
    "- get_text()是取得該標記/屬性所標註的文本內容\n",
    "- get_text()會剝掉所有的HTML標記，因為，對我們找語料而言，是好的；但是，如果要做一些需要標記的工作時，就不應使用了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他 beautifulSoup 物件\n",
    "\n",
    "- 目前，我們用過兩種beautifulSoup物件：\n",
    "  1. BeautifulSoup：就是把某網頁讀進來，並用HTML剖析器剖析網頁內容，供稍後使用的函式，這個物件通常簡寫為bs。\n",
    "  1. tag (標記)物件：在BeautifulSoup物件上呼叫find()/find_all()得到的，如bs.div.h1\n",
    "- 有兩個使用頻率沒那麼高，但也很重要的物件：\n",
    "  1. NavigableString 物件：代表標記內的文本，而非標記本身\n",
    "  1. Comment 物件：用來找HTML中的註解，如：<img src = 'commentOBJ.png'>\n",
    "- 以上四個，是beautifulSoup中最常使用的四個物件。\n",
    "\n",
    "## Navigating Trees\n",
    "\n",
    "- 可以在HTML結構樹中導航，是根據位置找到某標記的最佳方式。\n",
    "- 我們來試試看。先看網頁：http://www.pythonscraping.com/pages/page3.html。\n",
    "- 打開開發者工具，我們可以看到一層一層的階層：<br>\n",
    "  <img src ='htmlHierar.png'>\n",
    "### (樹狀結構中的)小孩(children)及後代 (descendants)\n",
    "\n",
    "- 如果你熟悉句法學的術語，小孩就是immediate dominance，後代就是dominance。\n",
    "- 整體而言，beautifulSoup的函式是處理某個標記的後代，而不僅限於小孩。\n",
    "- 比如說，hs.body.h1只會找到body後代中的第一個h1，不會去找body以外的範圍。\n",
    "- 如果只想要小孩這一層，可以用 .children："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'descendants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-330db98e79d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#for child in bs.find('table', {'id':'giftList'}).children:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'footer'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescendants\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'descendants'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#for child in bs.find('table', {'id':'giftList'}).children:\n",
    "for child in bs.find('table', {'id':'giftList'}).descendants:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with siblings\n",
    "\n",
    "- 如果要從table中擷取資料，特別是有標題列(title row)的表格，使用next_siblings()函式特別適合。\n",
    "- 下面的程式碼可以把id為giftList的表格中，擷取所有的列。但是不會擷取標題列。為什麼？因為next_siblings()已表明是擷明下一個兄弟姐妹了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'next_siblings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b72b78c94db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msibling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'footer'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_siblings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msibling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'next_siblings'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making selection specific\n",
    "\n",
    "- 上面的程式碼標明了：bs.find('table', {'id':'giftList'}).tr。不過，因為這個網頁只有一個表格，故使用bs.table.tr，甚至是bs.tr都可以達到相同效果。\n",
    "- 不過，為了讓你的爬蟲不要受到網頁改版的影響，如果網頁改版，增加表格，而你想要的表格不再是第一個，那麼不標明表格id的做法會擷取錯誤的表格。因此，在使用find()時，標記應該盡可能的標明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with parents\n",
    "\n",
    "- 擷取網頁資料少，較少要看親層，較常看子孫層。因為檢視網頁內容，都是從上層往下看的。\n",
    "- 不過如果真的有需要看親層，可以使用 .parent及 .parents。\n",
    "- 下面的碼，可以找出img的親層的上一個的前一個兄弟層的文件的文本。<br>\n",
    "  <img src='parentHierar.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$15.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.find('img', {'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression (正則表達式)\n",
    "\n",
    "- Regular expression是計算理論基礎，代表了最簡單的形式語言：單純的字母，以前後相連接、重覆、交集等方式形成的簡單語言。\n",
    "- 不過，很多時候，regular expression被視為是一種用來搜尋字串的公式。\n",
    "- Python可以使用正則表達式的模組叫：re。\n",
    "- re可以用來在已擷取的網頁中搜尋你想要、但又沒特定標記的內容。比如說，email address:<br>\n",
    "  [A-Za-z0-9\\.\\_+]+@[A-Zz-z]\\.(com|org|edu|net)\n",
    "- 你現在知道，為什麼想要避免你放在網頁上的email address被爬蟲擷取，有些人會用{@}取代@，甚至乾脆放個@的圖型在那兒了嗎？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prime Mini', 'Parliament', 'Parliament']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string = \"LONDON — Prime Minister Boris Johnson turned to Britain’s queen on Wednesday to limit Parliament’s ability to challenge his plan to take the country out of the European Union in nine weeks, with or without a deal. Mr. Johnson asked Queen Elizabeth II to suspend Parliament in September, a move that will cut the already dwindling number of days lawmakers have to find an alternative path ahead of the looming Brexit deadline on Oct. 31. The startling maneuver, Mr. Johnson’s boldest move since taking office a month ago, was immediately denounced by the opposition as undemocratic and possibly unconstitutional, and even a former prime minister in Mr. Johnson’s own Conservative party said the decision could be challenged in the courts.\"\n",
    "\n",
    "re.findall('^p|P.{3,9}', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression and beautifulSoup\n",
    "\n",
    "- 我們可以利用正則表達示放在find中，來達到全盤搜尋所有名稱中有特定字串的標記、內容。\n",
    "- 比如，如果我們要搜尋所有產品圖檔，應該怎麼做？\n",
    "- 如果使用 bs.find('img')，有可以找到logo或其他你沒想到、但卻出現在網頁上的圖檔。\n",
    "- 所以，..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "images = bs.find_all('img',\n",
    "                    {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "for image in images:\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Attributes\n",
    "\n",
    "- 除了在網頁中找尋標記及其文本內容外，我們也可以要找屬性(attributes)。\n",
    "- 比如，標記 a，其所指向的網址是在 href 屬性中；或是img標記，標的圖檔位於 src 屬性之中。\n",
    "- 可以使用下面方式：myTag.attrs來得到結果，如：myImgTag.attrs['src'] 可以取得圖檔下src的屬性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Expressions\n",
    "\n",
    "- lambda expression其實是一種函式，不過，不指派名稱，而是當成論元傳入另一個函式中，供後者做運算。\n",
    "- 比如：\n",
    "    1. bs.find_all(lambda tag: len(tag.attrs)==2) 取得所有只包含兩個屬性的標記\n",
    "    1. bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?') 等於 bs.find_all('', text='Or maybe he\\'s only resting?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課堂練習\n",
    "\n",
    "1. 把page3.html中，一開始的文字：Here is a collection... for gift wrapping.擷取出來。\n",
    "2. 到：books.toscrape.com，點選第一本書，把product desription下面的敍述擷取出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
